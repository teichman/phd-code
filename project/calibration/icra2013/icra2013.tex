

%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper
\usepackage[pdftex]{graphicx}

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


\usepackage{hyperref}
% The following packages can be found on http:\\www.ctan.org
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{multirow}
\usepackage[ruled,vlined,oldcommands]{algorithm2e}
\usepackage{algorithmic}

\title{\LARGE \bf
Unsupervised Calibration of Multiple Depth Sensors (TODO come up with real title)}
%TODO@SDM rename

\author{Alex Teichman, Stephen Miller, and Sebastian Thrun
\thanks{Alex Teichman, Stephen Miller, and Sebastian Thrun are with the Department of Computer Science at Stanford University. \newline E-mail: \{teichman, stephen, thrun\}@cs.stanford.edu } }



\include{latex_macros}
\newcommand{\img}{../img}
\newcommand{\simg}{static_img}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
aoeu
TODO
%TODO@SDM Write
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

%TODO@SDM Write
Until quite recently, depth data was a luxury available only to a handful of research labs. It would require 
either expensive equipment, such as LIDAR, or a finely calibrated pair of stereo cameras. With the recent 
introduction of inexpensive RGB-D sensors like the Microsoft Kinect \cite{kinect} or Asus Xtion Pro \cite{xtion}, 
however, virtually anyone can collect colored 3D pointclouds for the cost of a point-and-shoot camera.

The impact of RGB-D data is undeniable. Rather than discerning information from a rectangular grid of pixels, algorithms 
are able to reason in the intuitive world of Euclidian space---shape and relative distance can be precisely known. 
Problems which are very difficult in the 2D realm, such as segmentation and tracking, greatly benefit from depth 
information \cite{rgbdtracking}.  Others, such as classification and pose estimation, have shown great promise 
from the use of 3D models with corresponding keypoints \cite{rgbdclassification}.
And with the availability of datasets such as \cite{rgbd-dataset} and \cite{nist-dataset}, newer algorithms continue to 
be developed to exploit this new modality.

While these sensors provide fairly accurate depth readings, they are limited in their field of view. Rather than reasoning 
about entire objects or scenes, one must instead reason about a single viewpoint \ref{fig:viewpoint}, which is rarely 
an intuitive 3D shape. In order to 
construct accurate 3D models from a single camera, one must use a precisely calibrated rig to collect 
data \cite{solutionsinperception}, or else hold it stationary and move the camera, successively aligning frames til a 
full model is reconstructed. Both of these approaches, while successful, are limited in scope: the first requires a 
great amount of human intervention, and both require objects of interest to be rigid.

One can imagine instead a room with multiple RGB-D cameras, each with a novel view of the scene. With good 
extrinsic calibration---knowledge of the translation, rotation, and time offset of each sensor with respect to a world frame---
3D models of moving objects can be constructed on the fly, allowing algorithms to analyze dynamic 3D scenes. Such extrinsics 
are often difficult to get, however, requiring ground-truth correspondences by means of a precise calibration pattern 
\cite{checkerboard} or IR light source \cite{IRlight}. If any camera drifts or is bumped, as would likely be the case in 
a household or office setting, the calibration sequence must be redone. Like the aforementioned reconstruction techniques, 
intentional human effort is still required.

In this paper, we aim to make the extrinsic calibration task as painless and off-the-shelf as the sensors themselves, 
requiring no intentional human effort. Rather than relying on structured calibration patterns, 
we will use the scene itself to calibrate. As objects move within the scene, their motion provides automatic
correspondences between all sensors which view it, which is used to estimate extrinsics. 
The shape and appearance of these objects---as well as overlapping portions of the scene itself---can be used to further 
refine this estimate. We make no assumptions about the objects in the scene or the way they move, nor do we assume anything 
about the relative poses of the cameras, beyond the existence of \emph{some} overlap in field of view.

%TODO say how rest of the paper is organized

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{static_img/example_alignment.pdf}
  \caption{An example calibration task. \textbf{Left}: the image and associated point cloud for both sensors. \textbf{Right}: The desired combined cloud.}
  \label{fig:example_alignment}
\end{figure*}

\section{RELATED WORK}
\cite{rubleeorb}
%TODO@SDM Write
\subsection{Calibration}

\subsection{Reconstruction}

\subsection{RGB-D}


\section{OUR APPROACH}
TODO
%TODO@SDM Write




 we first assume that NTP or similar methods can provide synchronization to within about 100ms.  The core idea of the approach is to use the rough locations of moving objects to provide a reasonable starting point for ICP over transforms and grid search over synchronization offsets.

However, naively running ICP on entire frames can result in very poor alignments, an example of which can be seen in Figure \todo{x}.  This is because ICP rewards transforms in which many points are nearby, and floors often have more observed points on them than the people or robots walking on them.  Running ICP on just the foreground objects in each frame produces much better results.


\begin{algorithm}
  \caption{Algorithm sketch}
  \label{alg:sketch}
  \SetLine
  \KwData{Two roughly synchronized RGBD videos}
  \KwResult{Transform and synchronization offset}
  \phantom{\;}
  Background subtraction to find foreground points\;
  Connected components to find large objects\;
  $T$ = centroidRANSAC()\;
  Apply $T$ to floating scenes\;
  \While{$||T||_F > 0.001$}{
    $T$ = updateTransformICP()\;
    Apply $T$ to floating scenes\;
    $s$ = updateSync()\;
    Apply $s$ to floating scenes\;
  }
\end{algorithm}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{\simg/intuition/intuition.pdf}
  \caption{Background subtraction (foreground shown in red), object extraction, rough initialization from ransac, final result after running alternating sync search and ICP on all foreground models simultaneously. }
  \label{fig:}
\end{figure*}


background subtraction is a histogram of depth values for each pixel.  If at least some percent of the values in the histogram have ended up in that bin, then it's a background point.

connected components is the usual thing, with erode and dilate to get rid of small points.  objects were required to be at least 0.5m in one direction and 1000 points.

updateSync was done by grid search over
\newcommand{\dmax}{d_{\mbox{\tiny{max}}}}
\begin{align*}
  \frac{1}{M} \sum_{i=1}^M \frac{1}{N_i} \sum_{j=1}^{N_i}    (\dmax, ||p_j - p'||_2)
\end{align*}
where $i$ ranges over matched foreground scenes and $j$ over points in the floating sensor.  $p'$ is the nearest neighbor.


referred to as \texttt{centroidRANSAC()} in Algorithm~\ref{alg:sketch}, provides a rough estimate of the transform which serves as a reasonable starting point for ICP.


\begin{algorithm}
  \caption{Centroid RANSAC}
  \label{alg:cal}
  \SetLine
  \KwData{Two roughly synchronized sets of foreground objects from RGBD videos}
  \KwResult{Rough guess of transform}
  \phantom{\;}
  inlier threshold 0.5m.  RANSAC, 1000 samples of correspondences.
\end{algorithm}



\section{EXPERIMENTS}
TODO
%TODO@SDM Write
%TODO@SDM Try to see if ground truth can be better. Really seems like these errors aren't fair.

To keep the amount of data manageable but still produce a good result, we used between about thirty and ten percent of the total frames in each sequence, sampling chunks of consecutive frames to allow synchronization search to work.

Because of the availability of NTP, approximate synchronization of the sensors is possible across multiple machines.  However, NTP alone does not provide sufficient precision for aligning 30fps RBGD data, where offsets tens of milliseconds are significant.


sensors do not trigger at the same time, leading to unavoidable synchronization slop that we do not address.



\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{\img/bargraph.pdf}
  \caption{}
  \label{fig:bargraph}
\end{figure}

\begin{figure}
  \centering
  \input{tables/results_table.tex}
  \label{fig:results}
\end{figure}

Angular error results are reported as the angle in the angle-axis representation of the rotation matrix 

\section{CONCLUSIONS}
TODO
%TODO@SDM Write

\bibliographystyle{IEEEtran}
\bibliography{icra2013}

\end{document}

