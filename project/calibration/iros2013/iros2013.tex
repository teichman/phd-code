

%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper
\usepackage[pdftex]{graphicx}

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


\usepackage{hyperref}
% The following packages can be found on http:\\www.ctan.org
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{multirow}
\usepackage[ruled,vlined,oldcommands]{algorithm2e}
\usepackage{algorithmic}

\title{\LARGE \bf
Unsupervised Calibration of Multiple Depth Sensors (TODO come up with real title)}
%TODO@SDM rename

\author{Stephen Miller, Alex Teichman, and Sebastian Thrun
\thanks{Stephen Miller, Alex Teichman, and Sebastian Thrun are with the Department of Computer Science at Stanford University. \newline E-mail: \{teichman, stephen, thrun\}@cs.stanford.edu } }



\include{latex_macros}
\newcommand{\img}{img}
\newcommand{\simg}{static_img}
\newcommand{\tables}{img}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
aoeu
TODO
%TODO@SDM Write
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

%TODO@SDM Some fluff about the growth of RGB-D
With the recent introduction of inexpensive RGB-D sensors like the Microsoft Kinect \cite{kinect} or Asus Xtion Pro \cite{xtion}, virtually anyone 
can now collect colored 3D pointclouds for the cost of a point-and-shoot camera.

The impact of RGB-D data is undeniable. Rather than discerning information from a rectangular grid of pixels, algorithms 
are able to reason in the intuitive world of Euclidian space---shape and relative distance can be precisely known. 
This has impacted nearly every field of perception, be it reconstruction %TODO CITE, 
, segmentation and tracking %TODO CITE
, or object detection. %TODO CITE
And with the availability of datasets such as \cite{rgbd-dataset} and \cite{nist-dataset}, algorithms continue to 
be developed to exploit this new modality.

%TODO@SDM say something about the problem of 2.5 D, how this is still not intuitive

One can imagine instead a room with multiple RGB-D cameras, each with a novel view of the scene. With good 
extrinsic calibration---knowledge of the translation, rotation, and time offset of each sensor with respect to a world frame---
3D models of moving objects could be constructed on the fly, allowing algorithms to analyze dynamic 3D scenes. Such extrinsics 
are often difficult to get, however, requiring ground-truth correspondences by means of a precise calibration pattern 
\cite{checkerboard} or IR light source \cite{IRlight}. If any camera drifts or is bumped, as would likely be the case in 
a household or office setting, the calibration sequence must be redone. Like the aforementioned reconstruction techniques, 
intentional human effort is still required.

In this paper, we aim to make the extrinsic calibration task as painless and off-the-shelf as the sensors themselves, 
requiring no intentional human effort. Rather than relying on structured calibration patterns, 
we will use the scene itself to calibrate. As objects move within the scene, their motion provides
correspondences between all sensors which view it, which are used to estimate the extrinsics. 
The shape and appearance of these objects---as well as overlapping portions of the scene---can be used to further 
refine this estimate. We make no assumptions about the objects in the scene or the way they move, nor do we assume anything 
about the relative poses of the cameras, beyond the basic requirement that \emph{something} in the scene must be visible to every pair of cameras.

%TODO say how rest of the paper is organized

\begin{figure*}
  \centering
  %\includegraphics[width=\linewidth]{static_img/example_alignment.pdf}
  \caption{An example calibration task. \textbf{Left}: the image and associated point cloud for both sensors. \textbf{Right}: The desired combined cloud.}
  \label{fig:example_alignment}
\end{figure*}

\section{RELATED WORK}
\cite{rubleeorb}
%TODO@SDM Write
\subsection{Calibration}

\subsection{Reconstruction}

\subsection{RGB-D}


\section{OUR APPROACH}
TODO
%TODO@SDM Write

% Step 1: Extract foreground objects
% Step 2: Centroid RANSAC
% Step 3: Object-to-scene refinement
% Step 4: Time synchronization


 we first assume that NTP or similar methods can provide synchronization to within about 100ms.  T``he core idea of the approach is to use the rough locations of moving objects to provide a reasonable starting point for ICP over transforms and grid search over synchronization offsets.

However, naively running ICP on entire frames can result in very poor alignments, an example of which can be seen in Figure \todo{x}.  This is because ICP rewards transforms in which many points are nearby, and floors often have more observed points on them than the people or robots walking on them.  Running ICP on just the foreground objects in each frame produces much better results.


\begin{algorithm}
  \caption{Algorithm sketch}
  \label{alg:sketch}
  \SetLine
  \KwData{Two roughly synchronized RGBD videos}
  \KwResult{Transform and synchronization offset}
  \phantom{\;}
  Background subtraction to find foreground points\;
  Connected components to find large objects\;
  $T$ = centroidRANSAC()\;
  Apply $T$ to floating scenes\;
  \While{$||T||_F > 0.001$}{
    $T$ = updateTransformICP()\;
    Apply $T$ to floating scenes\;
    $s$ = updateSync()\;
    Apply $s$ to floating scenes\;
  }
\end{algorithm}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{\simg/intuition/intuition.pdf}
  \caption{Background subtraction (foreground shown in red), object extraction, rough initialization from ransac, final result after running alternating sync search and ICP on all foreground models simultaneously. }
  \label{fig:}
\end{figure*}


background subtraction is a histogram of depth values for each pixel.  If at least some percent of the values in the histogram have ended up in that bin, then it's a background point.

connected components is the usual thing, with erode and dilate to get rid of small points.  objects were required to be at least 0.5m in one direction and 1000 points.

updateSync was done by grid search over
\newcommand{\dmax}{d_{\mbox{\tiny{max}}}}
\begin{align*}
  \frac{1}{M} \sum_{i=1}^M \frac{1}{N_i} \sum_{j=1}^{N_i}    (\dmax, ||p_j - p'||_2)
\end{align*}
where $i$ ranges over matched foreground scenes and $j$ over points in the floating sensor.  $p'$ is the nearest neighbor.


referred to as \texttt{centroidRANSAC()} in Algorithm~\ref{alg:sketch}, provides a rough estimate of the transform which serves as a reasonable starting point for ICP.


\begin{algorithm}
  \caption{Centroid RANSAC}
  \label{alg:cal}
  \SetLine
  \KwData{Two roughly synchronized sets of foreground objects from RGBD videos}
  \KwResult{Rough guess of transform}
  \phantom{\;}
  inlier threshold 0.5m.  RANSAC, 1000 samples of correspondences.
\end{algorithm}



\section{EXPERIMENTS}
TODO
%TODO@SDM Write
%TODO@SDM Try to see if ground truth can be better. Really seems like these errors aren't fair.

To keep the amount of data manageable but still produce a good result, we used between about thirty and ten percent of the total frames in each sequence, sampling chunks of consecutive frames to allow synchronization search to work.

Because of the availability of NTP, approximate synchronization of the sensors is possible across multiple machines.  However, NTP alone does not provide sufficient precision for aligning 30fps RBGD data, where offsets tens of milliseconds are significant.


sensors do not trigger at the same time, leading to unavoidable synchronization slop that we do not address.



\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{\img/bargraph.pdf}
  \caption{}
  \label{fig:bargraph}
\end{figure}

\begin{figure}
  \centering
  \input{\img/results_table.tex}
  \label{fig:results}
\end{figure}

Angular error results are reported as the angle in the angle-axis representation of the rotation matrix 

\section{CONCLUSIONS}
TODO
%TODO@SDM Write

\bibliographystyle{IEEEtran}
\bibliography{icra2013}

\end{document}

